{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHERE WE EXTRACT TABLE NAME FROM CSV\n",
    "from bs4 import BeautifulSoup\n",
    "from tika import parser\n",
    "import os\n",
    "import re\n",
    "import camelot\n",
    "from fuzzywuzzy import fuzz\n",
    "from datetime import datetime\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################FUNCTIONS##################################################\n",
    "def get_table_titles(page:int,file,df:pd.DataFrame) -> list():#pd.DataFrame:\n",
    "    tbl_names_trunc = list()\n",
    "    tbl_names = list(df[(df['page_number']== page) & (df['DataID_pdf']== file)]['final_table_title'])\n",
    "    for tbl in tbl_names:\n",
    "        if len(tbl) > 218:\n",
    "            tbl_names_trunc.append(textwrap.shorten(tbl,width = 218))\n",
    "        else:\n",
    "            tbl_names_trunc.append(tbl)      \n",
    "    return tbl_names_trunc\n",
    "#********************************************************************************#\n",
    "def replace_chars(text:str) -> str:\n",
    "    chars_0 = ['\\n',':']\n",
    "    chars_1 = ['/','\\\\']\n",
    "    for c in chars_0:\n",
    "        text = text.replace(c, ' ')\n",
    "    for cc in chars_1:\n",
    "        text = text.replace(cc,'_')\n",
    "    return text\n",
    "#********************************************************************************#\n",
    "def replace_chars_strings(lst:list) -> list:\n",
    "    new_lst = []\n",
    "    for itm in lst:\n",
    "        new_lst.append(replace_chars(itm))       \n",
    "    return new_lst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Function\n",
    "def extract_tables(file:str(),df:pd.DataFrame) -> list:   \n",
    "    failed_pdf = []\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    file_path = 'F:/Environmental Baseline Data/Version 4 - Final/PDF/{}'.format(file)\n",
    "\n",
    "    file_name = file_path.split('/')[-1].replace('.pdf','')\n",
    "\n",
    "    try:\n",
    "        data = parser.from_file(file_path,xmlContent=True)\n",
    "        #raw_xml = parser.from_file('A6T2V6.pdf', xmlContent=True)\n",
    "\n",
    "        #xml tag <div> splitting point for pages\n",
    "        soup = BeautifulSoup(data['content'], 'lxml')\n",
    "        pages = soup.find_all('div', attrs={'class': 'page'})\n",
    "\n",
    "        title_dict = dict()\n",
    "        start_time = datetime.now()\n",
    "        for ind, page in enumerate (pages):\n",
    "            pg_num = ind\n",
    "            chars = []\n",
    "            #camelot table objects for each page of the pdf\n",
    "            try:\n",
    "                tables = camelot.read_pdf(file_path, pages = str(pg_num+1), flag_size=True, copy_text=['v'],strip_text = '\\n',line_scale=40, f = 'csv',flavour = 'stream')  #loop len(tables)\n",
    "            except:\n",
    "                continue\n",
    "            #get table names in page == pg_num by parsing get_table_titles() function\n",
    "            title_lst_raw = get_table_titles(pg_num+1,file,df)\n",
    "            title_lst = replace_chars_strings(title_lst_raw)\n",
    "            #get total number of table objects detected by Camelot in page == pg_num\n",
    "            tb_num = tables.n\n",
    "\n",
    "            #VIEW\n",
    "            print(title_lst)  \n",
    "            #if Camelot returns NO table on the page continue the loop and go to the next page\n",
    "            if tb_num == 0:\n",
    "                print(\"No table on page \"+ str(pg_num+1) + \" is detected\")\n",
    "                continue\n",
    "            #if whitespace of the detected table is larger than 69% of the entire table and there is only\n",
    "            #one table on that page, identify this as figure and continute the loop and go to the next page\n",
    "            elif tb_num == 1 and (tables[0].parsing_report)['whitespace'] > 69.0:\n",
    "                print(\"Page {} contains an image\".format((tables[0].parsing_report)['page'] ))  \n",
    "                continue\n",
    "            #in case only one table is present on the page,\n",
    "            elif tb_num == 1:\n",
    "                #this block distills the dataframe with proper column names\n",
    "                df_tb = tables[0].df\n",
    "                df_tb = df_tb.replace('/na', '_', regex = True)\n",
    "                colname = df_tb.iloc[0].str.replace('\\n',' ',regex=True)\n",
    "                df_tb.columns = colname\n",
    "                df_tb = df_tb[1:]   \n",
    "                #df_tb = df_tb.iloc[1:]\n",
    "                #in case no title is extracted from this page but we know that there is one table\n",
    "                if len(title_lst) == 0:\n",
    "                    #let's say we are on page number x and this if statement checks whether page number x-1 contianed a table or not\n",
    "                    #if the result is TRUE we assign the title of last table on previous page to this page's title-less table\n",
    "                    if (pg_num-1 in title_dict):\n",
    "                        #find the list of tables on the previous page\n",
    "                        lst_tbl = (title_dict.get(pg_num-1))[-1]\n",
    "                        lst_tbl_df = lst_tbl[1]\n",
    "                        #find similarity score of column names of table on page x and page x-1\n",
    "                        col_concat_curr = list(df_tb.columns.values)\n",
    "                        ccc = ''.join(col_concat_curr)\n",
    "                        col_join_curr = (ccc.replace(' ','')).replace('\\n','')\n",
    "                        col_concat_prev = list(lst_tbl_df.columns.values)\n",
    "                        ccp = ''.join(col_concat_prev)\n",
    "                        col_join_prev = (ccp.replace(' ','')).replace('\\n','')  \n",
    "                        ratio_similarity = fuzz.token_sort_ratio(col_join_curr, col_join_prev)\n",
    "                        #check if the columns of the last table on the previous page are the same as the table on this page\n",
    "                        if (len((set(lst_tbl_df.columns)).difference(set(df_tb.columns))) == 0) or (len(set(lst_tbl_df.columns))== len(set(df_tb.columns))) or (ratio_similarity > 89):\n",
    "                            xl_name = lst_tbl[2]\n",
    "                            chars.append([0,df_tb,xl_name])\n",
    "                            xlsx_name = file_name + '_' + xl_name +'_'+str(pg_num+1)+'_'+str(1)+ '.csv'\n",
    "                            df_tb.to_csv(xlsx_name, index = False, encoding='utf-8-sig')     \n",
    "                        else:\n",
    "                            xl_name = file_name\n",
    "                            chars.append([0,df_tb,xl_name])\n",
    "                            xlsx_name = file_name + '_' +str(pg_num+1)+'_'+str(1)+ '.csv'\n",
    "                            df_tb.to_csv(xlsx_name, index = False, encoding='utf-8-sig')\n",
    "                        title_dict[pg_num] = chars\n",
    "\n",
    "                    else:\n",
    "                        xl_name = file_name       \n",
    "                        chars.append([0,df_tb,xl_name])\n",
    "                        xlsx_name = file_name + '_' +str(pg_num+1)+'_'+str(1)+ '.csv'\n",
    "                        df_tb.to_csv(xlsx_name, index = False, encoding='utf-8-sig')\n",
    "                        title_dict[pg_num] = chars\n",
    "                else:\n",
    "                    xl_name = title_lst[0]\n",
    "        #                xl_name = xl_name.replace('/','_')\n",
    "        #                xl_name = xl_name.replace(':','')\n",
    "                    #store page number, index of the table, and its name in a dictionary\n",
    "                    chars.append([0,df_tb,xl_name])\n",
    "                    xlsx_name = file_name + '_' + xl_name +'_'+str(pg_num+1)+'_'+str(1)+ '.csv'\n",
    "                    df_tb.to_csv(xlsx_name, index = False, encoding='utf-8-sig')\n",
    "                    title_dict[pg_num] = chars\n",
    "\n",
    "            else:\n",
    "                if len(title_lst) >= tb_num :\n",
    "                    for j in range(0,tb_num):\n",
    "                        df_tb = tables[j].df\n",
    "                        df_tb = df_tb.replace('/na', '_', regex = True)\n",
    "                        df_tb.columns = df_tb.iloc[0].str.replace('\\n',' ',regex=True)\n",
    "                        df_tb = df_tb[1:]\n",
    "                        #df_tb = df_tb.iloc[1:]\n",
    "                        xl_name = title_lst[j]\n",
    "        #                    xl_name = xl_name.replace('/','_')\n",
    "        #                    xl_name = xl_name.replace(':','')\n",
    "\n",
    "                        #store page number, index of the table, and its name in a dictionary\n",
    "                        chars.append([j,df_tb,xl_name])\n",
    "\n",
    "                        xlsx_name = file_name + '_' + xl_name +'_'+str(pg_num+1)+'_'+str(j+1)+ '.csv'\n",
    "                        df_tb.to_csv(xlsx_name, index = False, encoding='utf-8-sig')\n",
    "\n",
    "                    title_dict[pg_num] = chars\n",
    "\n",
    "                elif len(title_lst) < tb_num :\n",
    "                    try:\n",
    "                        #first case: if the first table in the page is conitnuos of what was on the previous page\n",
    "                        df_tb = tables[0].df\n",
    "                        df_tb = df_tb.replace('/na', '_', regex = True)\n",
    "                        df_tb.columns = df_tb.iloc[0].str.replace('\\n',' ',regex=True)\n",
    "                        df_tb = df_tb[1:]\n",
    "                        #df_tb = df_tb.iloc[1:]\n",
    "\n",
    "                        if (pg_num-1 in title_dict):\n",
    "                            #find the list of tables on the previous page\n",
    "                            lst_tbl = (title_dict.get(pg_num-1))[-1]\n",
    "                            lst_tbl_df = lst_tbl[1]\n",
    "                            #check if the columns of the last table on the previous page are the same as the table on this page\n",
    "\n",
    "                            col_concat_curr = list(df_tb.columns.values)\n",
    "                            ccc = ''.join(col_concat_curr)\n",
    "                            col_join_curr = (ccc.replace(' ','')).replace('\\n','')\n",
    "                            col_concat_prev = list(lst_tbl_df.columns.values)\n",
    "                            ccp = ''.join(col_concat_prev)\n",
    "                            col_join_prev = (ccp.replace(' ','')).replace('\\n','')  \n",
    "                            ratio_similarity = fuzz.token_sort_ratio(ccc, ccp)\n",
    "\n",
    "                            if len((set(lst_tbl_df.columns)).difference(set(df_tb.columns))) == 0 or len(set(lst_tbl_df.columns))== len(set(df_tb.columns)) or ratio_similarity > 89:\n",
    "                                xl_name = lst_tbl[2]                           \n",
    "                                chars.append([0,df_tb,xl_name])                         \n",
    "                                xlsx_name = file_name + '_' + xl_name +'_'+str(pg_num+1)+'_'+str(1)+ '.csv'\n",
    "                                df_tb.to_csv(xlsx_name, index = False, encoding='utf-8-sig')\n",
    "\n",
    "                            else:\n",
    "                                xl_name = file_name   \n",
    "                                chars.append([0,df_tb,xl_name])\n",
    "                                xlsx_name = file_name + '_' +str(pg_num+1)+'_'+str(1)+ '.csv'\n",
    "                                df_tb.to_csv(xlsx_name, index = False, encoding='utf-8-sig')\n",
    "                        else:\n",
    "                            xl_name = file_name       \n",
    "                            chars.append([0,df_tb,xl_name])\n",
    "                            xlsx_name = file_name + '_' +str(pg_num+1)+'_'+str(1)+ '.csv'\n",
    "                            df_tb.to_csv(xlsx_name, index = False, encoding='utf-8-sig')\n",
    "\n",
    "                        indx = 0\n",
    "                        for j in range(1,len(title_lst)+1):\n",
    "                            df_tb = tables[j].df\n",
    "                            df_tb = df_tb.replace('/na', '_', regex = True)\n",
    "                            df_tb.columns = df_tb.iloc[0].str.replace('\\n',' ',regex=True)\n",
    "                            df_tb = df_tb[1:]\n",
    "                            #df_tb = df_tb.iloc[1:]\n",
    "                            xl_name = title_lst[indx]\n",
    "                            indx = indx + 1\n",
    "        #                        xl_name = xl_name.replace('/','_')\n",
    "        #                        xl_name = xl_name.replace(':','')\n",
    "                            #store page number, index of the table, and its name in a dictionary\n",
    "                            chars.append([j,df_tb,xl_name])\n",
    "\n",
    "                            xlsx_name = file_name + '_' + xl_name +'_'+str(pg_num+1)+'_'+str(j+1)+ '.csv'\n",
    "                            #xlsx_name = z.split('\\\\')[-1] + '-' + str(pg[i]+1) + str(j) + '.xlsx'\n",
    "                            df_tb.to_csv(xlsx_name, index = False, encoding='utf-8-sig')\n",
    "\n",
    "                        for j in range(len(title_lst)+1,tb_num):\n",
    "                            df_tb = tables[j].df\n",
    "                            df_tb = df_tb.replace('/na', '_', regex = True)\n",
    "                            df_tb.columns = df_tb.iloc[0].str.replace('\\n',' ',regex=True)\n",
    "                            df_tb = df_tb[1:]\n",
    "                            #df_tb = df_tb.iloc[1:]\n",
    "                            xl_name = file_name\n",
    "                            #store page number, index of the table, and its name in a dictionary\n",
    "                            chars.append([j,df_tb,xl_name])                        \n",
    "\n",
    "                            xlsx_name = file_name + '_' + xl_name +'_'+str(pg_num+1)+'_'+str(j+1)+ '.csv'\n",
    "                            #xlsx_name = z.split('\\\\')[-1] + '-' + str(pg[i]+1) + str(j) + '.xlsx'\n",
    "                            df_tb.to_csv(xlsx_name, index = False, encoding='utf-8-sig')                    \n",
    "                        title_dict[pg_num] = chars\n",
    "\n",
    "                    except:\n",
    "                        print(\"Function failed on page {}\".format(pg_num+1))\n",
    "                        pass\n",
    "    except:\n",
    "        failed_pdf.append(file)\n",
    "        print(\"file {} failed to open\".format(file))\n",
    "        pass\n",
    "        \n",
    "    end_time = datetime.now()\n",
    "    print('Duration: {}'.format(end_time - start_time)) \n",
    "    \n",
    "    return failed_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'F:/Environmental Baseline Data/Version 4 - Final/Support files/Table titles raw data/final_table_titles2.csv'\n",
    "df = pd.read_csv(path, usecols = ['page_number','final_table_title', 'Application title short', 'DataID_pdf', \\\n",
    "                                  'categories', 'Category'])\n",
    "df = df[df['categories'] > 0] \n",
    "df = df[df['Category'] == 'Table']\n",
    "df['final_table_title'] = df['final_table_title'].str.title()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application for 2021 NGTL System Expansion Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hearing = 'Application for 2021 NGTL System Expansion Project' \n",
    "ngtl2021 = df[df['Application title short'] == hearing].reset_index(drop = True)\n",
    "ngtl2021.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change this folder to the path were you want the tables saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'F:\\Environmental Baseline Data\\Version 4 - Final\\CSV\\ngtl2021')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the dataframe name accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(ngtl2021['DataID_pdf'].unique())\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### call the main function -- pass filename and dataframe as function arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    extract_tables(file,ngtl2021)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application for the 2017 NGTL System Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hearing = 'Application for the 2017 NGTL System Expansion'\n",
    "ngtl2017 = df[df['Application title short'] == hearing].reset_index(drop = True)\n",
    "ngtl2017.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change this folder to the path were you want the tables saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'F:\\Environmental Baseline Data\\Version 4 - Final\\CSV\\ngtl2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the dataframe name accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = list(ngtl2017['DataID_pdf'].unique())\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### call the main function -- pass filename and dataframe as function arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    extract_tables(file,ngtl2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application for Northwest Mainline Komie North Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hearing = 'Application for Northwest Mainline Komie North Extension'\n",
    "komie_north = df[df['Application title short'] == hearing].reset_index(drop = True)\n",
    "komie_north.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change this folder to the path were you want the tables saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'F:\\Environmental Baseline Data\\Version 4 - Final\\CSV\\komie_north')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the dataframe name accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(komie_north['DataID_pdf'].unique())\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### call the main function -- pass filename and dataframe as function arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    extract_tables(file,komie_north)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
